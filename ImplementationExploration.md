# T-MLP设计探索

## Motivation
我们的出发点是这样的，当前的深度学习离不开多层感知机MLP，而MLP实际上就是多个矩阵乘法加上中间的一些激活函数。由于涉及到大量矩阵运算，MLP往往成为了很多模块当中的计算瓶颈。而我们的这个工作就是尝试使用近似的方式来加速MLP的计算过程。

## Idea
MLP虽然听起来复杂，实际上就是一个复合函数，对于输入X，MLP可以看作F(X)=fn(fn-1(...f1(x)))，对于每一层都是一个矩阵乘加上一个激活函数f(X)=sigma(WX+B)。那么我们的想法很简单，通过使用泰勒展开近似的方式来近似一个MLP也就是使用F(X_0)+F'(X_0)(X-X_0)+...来近似一个MLP，从而将一个非常复杂的MLP近似成为一个简单的线性式子或者更复杂一点，近似成一个二次的式子。当然，近似的效果如何取决于我们找到的展开点X_0，越贴近展开点，自然近似越准确。因此，我们在训练数据集上对于输入采用Kmeans聚类的方法，得到一批展开点，这样，对于新的输入，在其最近的展开点展开，可以尽可能保证其准确率。

那么，总体来说，我们的方法就是将MLP替换成一个Kmeans聚类+泰勒展开式，从而极大地减少整体的计算量，并且尽可能保证近似的准确率。

**重要说明：整体MLP替换而非逐层替换**

需要特别强调的是，我们的方法是将整个MLP模块（包含所有线性层和激活函数）作为一个**整体复合函数**进行泰勒展开，而不是对MLP内部的每个线性层分别展开。

以VGG16的分类器为例：
```
原始MLP结构: fc6 → ReLU → fc7 → ReLU → fc8
            ↓
整体视为单一函数: F(X) = fc8(ReLU(fc7(ReLU(fc6(X)))))
            ↓
泰勒展开近似: F(X) ≈ F(X_0) + F'(X_0)(X - X_0)
```

这样做的好处：
- **存储效率**：只需存储整体Jacobian（输出维×输入维），而非各层Jacobian之和
- **近似精度**：直接近似最终输出，避免中间层误差累积
- **计算简化**：推理时直接查表，无需逐层计算

## 初步实验探索
我们已经进行了一些比较简单的实验，具体来讲是将VGG16模型的三层MLP展开到一阶，使用kmeans聚类提前计算的展开点从5逐步拓展到120。我们在CIFAR-100数据集上进行了实验，实验结果显示，如果只替换掉VGG16的MLP，我们可以在保证准确率几乎不损失的情况下，极大地提升速度，在树莓派这样的设备上甚至可以实现100倍以上的加速。我们的初步实验结果证明，我们的方法在一些场景下是极其有效的。但是当前的实验仍然不够完善，我们需要把他变成一个完整的实验，同时，探索其他模型的MLP替换结果，以及尝试二阶展开。

## 评价

从理论角度分析，该方案具有以下优势和挑战：

**优势：**
1. **计算效率显著提升**：通过将复杂的MLP计算替换为泰勒展开的线性或二次近似，大幅减少了矩阵乘法运算，特别适合资源受限的边缘设备。
2. **理论基础扎实**：泰勒展开是数学分析中的经典方法，对于光滑函数在局部区域的近似具有良好的理论保证。
3. **自适应展开点选择**：使用K-means聚类在训练数据上选择展开点，能够有效覆盖数据分布的主要区域，提高近似精度。

**挑战与局限：**
1. **近似误差累积**：MLP是多层复合函数，每层的近似误差会在深层网络中累积，可能导致整体性能下降。
2. **展开点数量与精度的权衡**：展开点数量增加会提高精度但降低计算效率，需要找到最优平衡点。
3. **高阶展开的复杂性**：二阶及以上展开虽然精度更高，但计算复杂度也会显著增加，可能抵消部分加速效果。
4. **数据分布敏感性**：方法对训练数据分布敏感，如果测试数据分布与训练数据差异较大，近似效果可能显著下降。

**总体评价**：该方案在理论上是可行的，特别适用于对计算效率要求高、对精度要求相对宽松的场景。初步实验结果（VGG16在CIFAR-100上的成功）验证了方法的有效性。建议后续重点研究误差累积机制、展开点优化策略以及高阶展开的实用性。


## 初步roadmap
我们的实验大体上要包含这些部分，分别替换卷积神经网络，以及transformer的MLP。当然，重点是后者。我们需要分别测试一阶，二阶的效果。那么我们需要一步一步来。分成下面几步走：
1. 数据准备
2. 模型源码修改及适配
3. 模型权重准备
4. 中间结果收集
5. Kmeans训练
6. 实验验证


## 一些可能的优化方案

### 1. 在线K-means（适合流式数据）

针对大规模训练数据集（如LLM预训练数据可能包含数千亿token），传统的批量K-means聚类在计算和存储上都面临挑战。在线K-means提供了一种增量式解决方案：

**核心思想**：
- 无需存储所有训练样本，流式处理数据
- 动态维护展开点集合，固定内存占用

**算法流程**：
```
初始化: 随机选择k个样本作为初始展开点

对于每个新样本x:
    找到最近展开点 c_nearest
    如果 distance(x, c_nearest) > threshold:
        以一定概率将x加入展开点集（或替换最不活跃的展开点）
    否则:
        更新 c_nearest = (1-α)×c_nearest + α×x  # 指数移动平均
```

**优势**：
- 内存占用固定，与训练数据规模无关
- 适合LLM预训练级别的流式数据场景
- 可适应数据分布的渐进变化
**适用场景**：
- 超大规模数据集（>100M样本）
- 持续学习/在线学习场景
- 内存受限的边缘设备预处理

### 2. 对角Hessian近似（二阶展开优化）

二阶泰勒展开理论上可以提供更精确的近似，但完整的Hessian张量存储成本极高。对角Hessian近似是一种实用的折中方案。

**存储成本对比（以DeiT-Base为例，d=768）**：

| 方案 | 维度 | k=100单展开点 | k=100总存储(12层) |
|-----|------|--------------|-------------------|
| 一阶(Jacobian) | 768×768 | 2.36 MB | 2.83 GB |
| 二阶(完整Hessian) | 768×768×768 | 1.81 GB | 2.17 TB |
| 二阶(对角Hessian) | 768×768 | 2.36 MB | 2.83 GB |

**核心思想**：
- 只保留Hessian的对角线元素（每个输出维度对自身的二阶导数）
- 假设不同输入维度之间无交叉二阶效应
- 存储复杂度从$O(d^3)$降至$O(d^2)$

**二阶展开公式（对角近似）**：

$$F(X) \approx F(X_0) + J(X_0)(X - X_0) + \frac{1}{2}\sum_i H_{ii}(X_0)(X_i - X_{0i})^2$$

其中$H_{ii}$是对角Hessian元素。

**优势**：
- 存储与一阶展开同量级（相同维度下）
- 保留部分曲率信息，精度高于纯一阶
- 计算简单，推理时仅增加逐元素乘法

**适用场景**：
- 需要比一阶更高精度但无法接受完整Hessian存储的场景
- DeiT-Small等小维度模型（384维，k=100对角Hessian仅需707MB）
- 作为完整二阶展开的baseline对比

**局限性**：
- 忽略了输入维度间的交叉二阶效应
- 对于强非线性耦合的MLP层，近似精度可能不足


